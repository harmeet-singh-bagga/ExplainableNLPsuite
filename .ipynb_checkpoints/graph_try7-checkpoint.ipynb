{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e826fcb-e1ff-4247-804e-adb58b60af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision transformers timm sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e772c1c6-6468-42b2-b358-a62be2e369af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7cd0a5b-410c-4e58-8554-036441379fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2c215d1c6b46958e7cebcd6c012621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383af7edec8d4201934f54ef51f0d873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_minicpm.py:   0%|          | 0.00/3.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72521c01a1f24cc1a800f21f752d0172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_navit_siglip.py:   0%|          | 0.00/41.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2_6:\n",
      "- modeling_navit_siglip.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2_6:\n",
      "- configuration_minicpm.py\n",
      "- modeling_navit_siglip.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17715bbf61a4ef2a79dcbe12da78cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_minicpmv.py:   0%|          | 0.00/15.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40f43d0d5e44c92abc2136fae6841dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "resampler.py:   0%|          | 0.00/34.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2_6:\n",
      "- resampler.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2_6:\n",
      "- modeling_minicpmv.py\n",
      "- resampler.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a320ab1c579446418df6099367b563c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/66.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c20af77df54dadad66ed78c7b4a477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38cf16b297646598a451dafa9741993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b51c59729834af3b17bb9f174235e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aceccd546f2495fa069e2a527519ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276b69b0d47343c0adf64274b366248b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8c534d105a4c8dafbba0f2bf590a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86534db4fbb346b0a35b0402eb4c7f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0886a55a9748d0b851b74f226ec9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.64k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655878469b8a4b90baa2f23839e4d312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_minicpmv_fast.py:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2_6:\n",
      "- tokenization_minicpmv_fast.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42569842f27644f1bca6b5becbeded21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bd596ae0ce46b1abec09a2e0b52023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf2913a186f4795af2a5096ad38ed08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e21e3ce6b147e594fce169f2e9ceff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646effbdec6d49f9bf2f2fb7b30f8219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the MiniCPM model and tokenizer\n",
    "model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', \n",
    "                                  trust_remote_code=True,\n",
    "                                  attn_implementation='sdpa', \n",
    "                                  torch_dtype=torch.bfloat16).eval().cuda()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fb371a-32dd-461f-b33c-90a7259f1224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1477a1bc9484b58b01a3a0340542e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/714 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce40f80d6047480eb946f6facef32f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing_minicpmv.py:   0%|          | 0.00/10.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20624bd20ec34dc89a66151ee5218f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "image_processing_minicpmv.py:   0%|          | 0.00/16.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2_6:\n",
      "- image_processing_minicpmv.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2_6:\n",
      "- processing_minicpmv.py\n",
      "- image_processing_minicpmv.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:520: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Response: The image shows a line graph depicting temperature changes over 31 days. The x-axis represents the days, ranging from 1 to 31, and the y-axis represents temperature in degrees (T). From the graph, it is evident that the temperature fluctuates throughout the period, with some peaks reaching close to 12 degrees and others dipping below 4 degrees. This suggests variability in the climate or environmental conditions being measured during this time frame.\n",
      "The image shows a temperature graph over 31 days. From the data, it appears that the temperature fluctuates significantly throughout the period, with notable peaks and troughs. The highest temperatures are observed around day 5 and day 30, while the lowest temperatures occur near day 4 and day 29. This pattern suggests variability in temperature conditions during this timeframe."
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "image_path = '/mnt/code/test_l1.png'  # Replace with the actual image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Define the question and the message structure\n",
    "question = 'What insights can be derived from the image?'\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "# Run the model inference\n",
    "res = model.chat(\n",
    "    image=None,         # Since MiniCPM may not accept images directly, we pass the image within msgs\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f'Assistant Response: {res}')\n",
    "\n",
    "# Optional: Using streaming for generating text\n",
    "generated_text = \"\"\n",
    "for new_text in model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    stream=True\n",
    "):\n",
    "    generated_text += new_text\n",
    "    print(new_text, flush=True, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900c684d-8b45-4335-a6f1-55dee5a7edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Response: The image is a bar chart showing the percentage of respondents who reported various conditions affecting their quality of life. It indicates that neuropathy had the highest impact, followed by bacterial infection and area-related issues. The data suggests that these conditions significantly affect people's daily lives, with neuropathy being the most severe according to this survey.\n",
      "The image is a bar chart that provides insights into the percentage of R50 cases associated with different conditions. The conditions listed are Site, Ischaemia, Neuropathy, Bacterial Infection, Area, and Depth. From the chart, it's evident that Neuropathy has the highest percentage at 84.6%, followed by Bacterial Infection at 49.1%. This suggests that these two conditions have a significant association with R50 cases as per the NEASYS method used in the study or analysis represented by this chart."
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "image_path = '/mnt/code/test_b2.png'  # Replace with the actual image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Define the question and the message structure\n",
    "question = 'What insights can be derived from the image?'\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "# Run the model inference\n",
    "res = model.chat(\n",
    "    image=None,         # Since MiniCPM may not accept images directly, we pass the image within msgs\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f'Assistant Response: {res}')\n",
    "\n",
    "# Optional: Using streaming for generating text\n",
    "generated_text = \"\"\n",
    "for new_text in model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    stream=True\n",
    "):\n",
    "    generated_text += new_text\n",
    "    print(new_text, flush=True, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6274673b-9ac7-48fc-93f0-d9d730e8c0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Response: The pie chart shows the distribution of three categories in relation to a certain percentage, which is 76.5%. The largest segment represents 'Ocupat' with 15.9%, followed by 'Aturat' at 6.5%, and 'Inactiu' at 76.5%. This suggests that the majority (76.5%) falls under the category of 'Inactiu', while 'Ocupat' and 'Aturat' are significantly smaller segments.\n",
      "The pie chart shows the distribution of a certain percentage (76.8%) in three categories: Ocupat, Aturat, and Inactiu. The largest portion is attributed to Ocupat at 76.8%, followed by Aturat at 15.9%, and Inactiu at 6.5%. This suggests that the majority of the subjects or entities being measured are categorized as 'Ocupat', while smaller percentages fall into 'Aturat' and 'Inactiu'."
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "image_path = '/mnt/code/test_p3.png'  # Replace with the actual image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Define the question and the message structure\n",
    "question = 'What insights can be derived from the image?'\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "# Run the model inference\n",
    "res = model.chat(\n",
    "    image=None,         # Since MiniCPM may not accept images directly, we pass the image within msgs\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f'Assistant Response: {res}')\n",
    "\n",
    "# Optional: Using streaming for generating text\n",
    "generated_text = \"\"\n",
    "for new_text in model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    stream=True\n",
    "):\n",
    "    generated_text += new_text\n",
    "    print(new_text, flush=True, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89c4799-efac-42cd-986e-3e5bac23c6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Response: The image is a historical chart titled \"Carte Figurative de la Température des Plaines de l'Europe, 1812-1813\" which translates to \"Figuretive Map of Temperature in the European Plains from 1812 to 1813.\" It appears to show temperature variations across different regions during this period. The map includes geographical names and possibly indicates how temperatures changed over time or between different locations. This kind of historical data can provide insights into climate patterns, weather conditions, and potentially the impact on people's lives during that era.\n",
      "The image appears to be a historical document or chart, possibly related to temperature data from 1812-1813. The text is in French and mentions locations such as \"Moscou\" (Moscow) and \"Vilna\" (Vilnius). It might be an old graph showing temperature variations over time for these regions during that period. This kind of information can provide insights into climatic conditions, weather patterns, and historical events that may have affected the areas depicted."
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "image_path = '/mnt/code/test_random.png'  # Replace with the actual image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Define the question and the message structure\n",
    "question = 'What insights can be derived from the image?'\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "# Run the model inference\n",
    "res = model.chat(\n",
    "    image=None,         # Since MiniCPM may not accept images directly, we pass the image within msgs\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f'Assistant Response: {res}')\n",
    "\n",
    "# Optional: Using streaming for generating text\n",
    "generated_text = \"\"\n",
    "for new_text in model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    stream=True\n",
    "):\n",
    "    generated_text += new_text\n",
    "    print(new_text, flush=True, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b1092b4-aa1b-4326-8883-c5a48a54e682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Response: The image is a detailed performance comparison chart for various graphics cards (GCs) running on the RTX 3080 architecture, specifically focusing on their performance in different games at high settings. Here are some key insights derived from this chart:\n",
      "\n",
      "1. **Performance Variability**:\n",
      "   - The chart shows that the performance of different GCs varies significantly across various games. For instance, in \"Anno 1800,\" the Nvidia GeForce RTX 4090 outperforms all other GCs, while in \"Battlefield V,\" the AMD Radeon RX 7900 XT3 leads.\n",
      "   \n",
      "2. **General Trends**:\n",
      "   - Generally, higher-end GCs tend to perform better in most games compared to lower-end ones.\n",
      "   - There are exceptions where mid-range or even entry-level GCs can outperform more expensive options in certain games.\n",
      "\n",
      "3. **Average FPS**:\n",
      "   - An average frame rate (FPS) is provided for each game, giving an overall sense of how well each GC performs across multiple benchmarks.\n",
      "   - The chart also includes an \"AVERAGE\" column showing the mean FPS for each game, which helps in understanding the general performance trend for each GC.\n",
      "\n",
      "4. **Power Consumption and Price**:\n",
      "   - Power consumption data is included, measured in Watts, which indicates how much energy each GC uses during gameplay.\n",
      "   - The price per average frame (PFAF) is listed, reflecting the cost efficiency of each GC based on its performance relative to its price.\n",
      "\n",
      "5. **Ranking**:\n",
      "   - At the bottom, there's a ranking (\"Rank (Best FPS Bang For Buck)\") based on the PFAF score, indicating which GC offers the best balance between performance and price.\n",
      "\n",
      "6. **Comparison Across Different Game Settings**:\n",
      "   - Games are set to highest settings, ensuring that the results reflect real-world gaming scenarios with maximum graphical demands.\n",
      "\n",
      "This chart is useful for gamers and tech enthusiasts looking to make informed decisions about which graphics card would be best suited for their specific gaming needs, considering factors like performance, power usage, and cost.\n",
      "The image is a detailed comparison chart for the RTX 3080 graphics card, showcasing its performance across various games and benchmarks compared to other Nvidia models. Here are some insights derived from the chart:\n",
      "\n",
      "1. **Benchmark Performance**:\n",
      "   - The RTX 3080 generally outperforms other Nvidia cards in most benchmark tests, as indicated by higher scores (green bars).\n",
      "   - It achieves an average boost clock speed of 1879 MHz, which is significantly higher than other models.\n",
      "   - In terms of frame rates, the RTX 3080 often reaches or exceeds 60 FPS in many games, indicating smooth gameplay.\n",
      "\n",
      "2. **Game-Specific Performance**:\n",
      "   - In games like Anno 1800, Assassin’s Creed Odyssey, Battlefield V, Borderlands 3, Civilization VI, Death Stranding, Detroit: Become Human, Devil May Cry 5, Doom Eternal, Far Cry 5, Gears 5, Hitman 2, Metro Exodus, Project Cars 3, Red Dead Redemption 2, Sekiro: Shadows Die Twice, Shadow of the Tomb Raider, Star Wars: Jedi Fallen Order, Strange Brigade, The Witcher 3, it consistently shows high performance with frame rates mostly above 60 FPS and sometimes exceeding 144 FPS.\n",
      "   - Games like Death Stranding show a slight decrease in performance compared to other titles, possibly due to specific optimizations or game engine limitations.\n",
      "\n",
      "3. **Power Consumption**:\n",
      "   - The RTX 3080 has a power consumption of 372W, making it one of the more power-hungry cards listed but still within the range typical for high-performance GPUs.\n",
      "\n",
      "4. **Price and Value**:\n",
      "   - The price per average frame rate at $3.81 is relatively low, suggesting good value for money.\n",
      "   - Compared to other models, the RTX 3080 offers better performance relative to its price, especially when considering the higher frame rates and power efficiency.\n",
      "\n",
      "5. **Ranking**:\n",
      "   - Based on FPS Bang For Buck, the RTX 3080 ranks third, indicating that while it's highly capable, there are other options that offer slightly better value for performance.\n",
      "\n",
      "This chart provides a comprehensive overview of the RTX 3080's capabilities, highlighting its strengths in gaming performance and its position in the market compared to other Nvidia models."
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "image_path = '/mnt/code/test_table.jpg'  # Replace with the actual image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Define the question and the message structure\n",
    "question = 'What insights can be derived from the image? Describe in detail your findings'\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "# Run the model inference\n",
    "res = model.chat(\n",
    "    image=None,         # Since MiniCPM may not accept images directly, we pass the image within msgs\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f'Assistant Response: {res}')\n",
    "\n",
    "# Optional: Using streaming for generating text\n",
    "generated_text = \"\"\n",
    "for new_text in model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    stream=True\n",
    "):\n",
    "    generated_text += new_text\n",
    "    print(new_text, flush=True, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f60e94-728e-4877-9553-60a1f80e8ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
