{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4ed6b4-d914-40fe-9ece-60879a0c302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Set ImageNet Mean and Std for normalization\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# Confirm PyTorch version\n",
    "print(f'Torch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f9bfc6-af7b-4c3c-9d56-8a6433c4209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the image transformation pipeline\n",
    "def build_transform(input_size):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "# Function to find closest aspect ratio\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "# Function for dynamic image preprocessing\n",
    "def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # Calculate the target aspect ratio\n",
    "    target_ratios = set((i, j) for n in range(min_num, max_num + 1)\n",
    "                        for i in range(1, n + 1)\n",
    "                        for j in range(1, n + 1)\n",
    "                        if i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "    target_width, target_height = image_size * target_aspect_ratio[0], image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    \n",
    "    return processed_images\n",
    "\n",
    "# Function to load and process the image\n",
    "def load_image(image_file, input_size=448, max_num=6):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c89335b-60a4-48b9-a918-3662b1c762e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n",
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7d69342d364e5bacab0cafcb74dd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "path = 'OpenGVLab/InternVL2-8B'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "\n",
    "# Set generation configuration\n",
    "generation_config = dict(\n",
    "    num_beams=1,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6902b61c-45d1-47cd-911c-1bd3bc3b1cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Analyze this graph\n",
      "Assistant: This graph shows the temperature over a period of 31 days. Here's a detailed analysis:\n",
      "\n",
      "1. **Initial Phase (Days 1-5):**\n",
      "   - The temperature starts at around 7°C and gradually decreases.\n",
      "   - By day 5, the temperature drops to approximately 2°C.\n",
      "\n",
      "2. **Rapid Decrease (Days 5-7):**\n",
      "   - There is a sharp decline in temperature, dropping from around 2°C to a low of about 1°C by day 7.\n",
      "\n",
      "3. **Recovery Phase (Days 7-10):**\n",
      "   - The temperature begins to recover, rising back up to around 6°C by day 10.\n",
      "\n",
      "4. **Fluctuating Phase (Days 10-20):**\n",
      "   - The temperature fluctuates significantly during this period. It rises to a peak of about 11°C around day 11, then drops back down to around 6°C by day 14.\n",
      "   - There are several peaks and troughs, with the temperature oscillating between 6°C and 11°C.\n",
      "\n",
      "5. **Stable Phase (Days 20-25):**\n",
      "   - The temperature stabilizes around 6°C to 8°C during this period, with minor fluctuations.\n",
      "\n",
      "6. **Final Increase (Days 25-31):**\n",
      "   - There is a noticeable increase in temperature towards the end of the period. By day 31, the temperature rises to approximately 12°C.\n",
      "\n",
      "Overall, the graph depicts a fluctuating temperature pattern with a significant drop initially, followed by recovery and stabilization, and a final increase towards the end of the 31-day period.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess an example image, moving pixel values to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pixel_values = load_image('/mnt/code/test_l1.png', max_num=6).to(torch.bfloat16).to(device)\n",
    "model.to(device)  # Ensure model is on the GPU\n",
    "\n",
    "# Run the model inference\n",
    "question = 'Analyze this graph'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "\n",
    "print(f'User: {question}')\n",
    "print(f'Assistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f0923-35d6-40e2-b4a9-ef58fd99929e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
