{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harmeet-singh-bagga/ExplainableNLPsuite/blob/main/graph_try7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8e826fcb-e1ff-4247-804e-adb58b60af95",
      "metadata": {
        "id": "8e826fcb-e1ff-4247-804e-adb58b60af95"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision transformers timm sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e772c1c6-6468-42b2-b358-a62be2e369af",
      "metadata": {
        "id": "e772c1c6-6468-42b2-b358-a62be2e369af"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a7cd0a5b-410c-4e58-8554-036441379fd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "a7cd0a5b-410c-4e58-8554-036441379fd9",
        "outputId": "b3c477cd-7ebe-4b7c-b666-d4652fd9c399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/openbmb/MiniCPM-V-2_6.\n401 Client Error. (Request ID: Root=1-672da909-3c5b4d7f0a407dba1c06fa0e;25341915-b21c-4576-b6e0-c6fbf773683c)\n\nCannot access gated repo for url https://huggingface.co/openbmb/MiniCPM-V-2_6/resolve/main/config.json.\nAccess to model openbmb/MiniCPM-V-2_6 is restricted. You must have access to it and be authenticated to access it. Please log in.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/openbmb/MiniCPM-V-2_6/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1241\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1854\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1855\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1753\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1675\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    320\u001b[0m             )\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-672da909-3c5b4d7f0a407dba1c06fa0e;25341915-b21c-4576-b6e0-c6fbf773683c)\n\nCannot access gated repo for url https://huggingface.co/openbmb/MiniCPM-V-2_6/resolve/main/config.json.\nAccess to model openbmb/MiniCPM-V-2_6 is restricted. You must have access to it and be authenticated to access it. Please log in.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-78acd0afe899>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize the MiniCPM model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', \n\u001b[0m\u001b[1;32m      3\u001b[0m                                   \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   \u001b[0mattn_implementation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sdpa'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   torch_dtype=torch.bfloat16).eval().cuda()\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    690\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresolved_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/openbmb/MiniCPM-V-2_6.\n401 Client Error. (Request ID: Root=1-672da909-3c5b4d7f0a407dba1c06fa0e;25341915-b21c-4576-b6e0-c6fbf773683c)\n\nCannot access gated repo for url https://huggingface.co/openbmb/MiniCPM-V-2_6/resolve/main/config.json.\nAccess to model openbmb/MiniCPM-V-2_6 is restricted. You must have access to it and be authenticated to access it. Please log in."
          ]
        }
      ],
      "source": [
        "# Initialize the MiniCPM model and tokenizer\n",
        "model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6',\n",
        "                                  trust_remote_code=True,\n",
        "                                  attn_implementation='sdpa',\n",
        "                                  torch_dtype=torch.bfloat16).eval().cuda()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "53657bfa-ad54-4dd4-a2cc-842eb6a12b04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53657bfa-ad54-4dd4-a2cc-842eb6a12b04",
        "outputId": "94eb3e2e-3c06-417b-8c3d-10d422b752b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4fb371a-32dd-461f-b33c-90a7259f1224",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d1477a1bc9484b58b01a3a0340542e52",
            "ce40f80d6047480eb946f6facef32f93",
            "20624bd20ec34dc89a66151ee5218f34"
          ]
        },
        "id": "b4fb371a-32dd-461f-b33c-90a7259f1224",
        "outputId": "dec40f78-d15f-4a56-97cf-db26d412485d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1477a1bc9484b58b01a3a0340542e52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/714 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce40f80d6047480eb946f6facef32f93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "processing_minicpmv.py:   0%|          | 0.00/10.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20624bd20ec34dc89a66151ee5218f34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "image_processing_minicpmv.py:   0%|          | 0.00/16.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2_6:\n",
            "- image_processing_minicpmv.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/openbmb/MiniCPM-V-2_6:\n",
            "- processing_minicpmv.py\n",
            "- image_processing_minicpmv.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:520: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant Response: The image shows a line graph depicting temperature changes over 31 days. The x-axis represents the days, ranging from 1 to 31, and the y-axis represents temperature in degrees (T). From the graph, it is evident that the temperature fluctuates throughout the period, with some peaks reaching close to 12 degrees and others dipping below 4 degrees. This suggests variability in the climate or environmental conditions being measured during this time frame.\n",
            "The image shows a temperature graph over 31 days. From the data, it appears that the temperature fluctuates significantly throughout the period, with notable peaks and troughs. The highest temperatures are observed around day 5 and day 30, while the lowest temperatures occur near day 4 and day 29. This pattern suggests variability in temperature conditions during this timeframe."
          ]
        }
      ],
      "source": [
        "# Load and preprocess the image\n",
        "image_path = '/mnt/code/test_l1.png'  # Replace with the actual image path\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# Define the question and the message structure\n",
        "question = 'What insights can be derived from the image?'\n",
        "msgs = [{'role': 'user', 'content': [image, question]}]\n",
        "\n",
        "# Run the model inference\n",
        "res = model.chat(\n",
        "    image=None,         # Since MiniCPM may not accept images directly, we pass the image within msgs\n",
        "    msgs=msgs,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(f'Assistant Response: {res}')\n",
        "\n",
        "# Optional: Using streaming for generating text\n",
        "generated_text = \"\"\n",
        "for new_text in model.chat(\n",
        "    image=None,\n",
        "    msgs=msgs,\n",
        "    tokenizer=tokenizer,\n",
        "    sampling=True,\n",
        "    stream=True\n",
        "):\n",
        "    generated_text += new_text\n",
        "    print(new_text, flush=True, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "900c684d-8b45-4335-a6f1-55dee5a7edbc",
      "metadata": {
        "id": "900c684d-8b45-4335-a6f1-55dee5a7edbc",
        "outputId": "682a19b9-d35c-4c2f-a58f-e75967d841ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant Response: The image is a bar chart showing the percentage of respondents who reported various conditions affecting their quality of life. It indicates that neuropathy had the highest impact, followed by bacterial infection and area-related issues. The data suggests that these conditions significantly affect people's daily lives, with neuropathy being the most severe according to this survey.\n",
            "The image is a bar chart that provides insights into the percentage of R50 cases associated with different conditions. The conditions listed are Site, Ischaemia, Neuropathy, Bacterial Infection, Area, and Depth. From the chart, it's evident that Neuropathy has the highest percentage at 84.6%, followed by Bacterial Infection at 49.1%. This suggests that these two conditions have a significant association with R50 cases as per the NEASYS method used in the study or analysis represented by this chart."
          ]
        }
      ],
      "source": [
        "# Load and preprocess the image\n",
        "image_path = '/mnt/code/test_b2.png'  # Replace with the actual image path\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# Define the question and the message structure\n",
        "question = 'What insights can be derived from the image?'\n",
        "msgs = [{'role': 'user', 'content': [image, question]}]\n",
        "\n",
        "# Run the model inference\n",
        "res = model.chat(\n",
        "    image=None,         # Since MiniCPM may not accept images directly, we pass the image within msgs\n",
        "    msgs=msgs,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(f'Assistant Response: {res}')\n",
        "\n",
        "# Optional: Using streaming for generating text\n",
        "generated_text = \"\"\n",
        "for new_text in model.chat(\n",
        "    image=None,\n",
        "    msgs=msgs,\n",
        "    tokenizer=tokenizer,\n",
        "    sampling=True,\n",
        "    stream=True\n",
        "):\n",
        "    generated_text += new_text\n",
        "    print(new_text, flush=True, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6274673b-9ac7-48fc-93f0-d9d730e8c0a0",
      "metadata": {
        "id": "6274673b-9ac7-48fc-93f0-d9d730e8c0a0",
        "outputId": "a687f6fc-0f69-4d48-830d-5c69c4d5ee96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant Response: The pie chart shows the distribution of three categories in relation to a certain percentage, which is 76.5%. The largest segment represents 'Ocupat' with 15.9%, followed by 'Aturat' at 6.5%, and 'Inactiu' at 76.5%. This suggests that the majority (76.5%) falls under the category of 'Inactiu', while 'Ocupat' and 'Aturat' are significantly smaller segments.\n",
            "The pie chart shows the distribution of a certain percentage (76.8%) in three categories: Ocupat, Aturat, and Inactiu. The largest portion is attributed to Ocupat at 76.8%, followed by Aturat at 15.9%, and Inactiu at 6.5%. This suggests that the majority of the subjects or entities being measured are categorized as 'Ocupat', while smaller percentages fall into 'Aturat' and 'Inactiu'."
          ]
        }
      ],
      "source": [
        "# Load and preprocess the image\n",
        "image_path = '/mnt/code/test_p3.png'  # Replace with the actual image path\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# Define the question and the message structure\n",
        "question = 'What insights can be derived from the image?'\n",
        "msgs = [{'role': 'user', 'content': [image, question]}]\n",
        "\n",
        "# Run the model inference\n",
        "res = model.chat(\n",
        "    image=None,         # Since MiniCPM may not accept images directly, we pass the image within msgs\n",
        "    msgs=msgs,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(f'Assistant Response: {res}')\n",
        "\n",
        "# Optional: Using streaming for generating text\n",
        "generated_text = \"\"\n",
        "for new_text in model.chat(\n",
        "    image=None,\n",
        "    msgs=msgs,\n",
        "    tokenizer=tokenizer,\n",
        "    sampling=True,\n",
        "    stream=True\n",
        "):\n",
        "    generated_text += new_text\n",
        "    print(new_text, flush=True, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c89c4799-efac-42cd-986e-3e5bac23c6f5",
      "metadata": {
        "id": "c89c4799-efac-42cd-986e-3e5bac23c6f5",
        "outputId": "c2ab9717-431d-412e-ebcc-695defbb1d03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant Response: The image is a historical chart titled \"Carte Figurative de la Température des Plaines de l'Europe, 1812-1813\" which translates to \"Figuretive Map of Temperature in the European Plains from 1812 to 1813.\" It appears to show temperature variations across different regions during this period. The map includes geographical names and possibly indicates how temperatures changed over time or between different locations. This kind of historical data can provide insights into climate patterns, weather conditions, and potentially the impact on people's lives during that era.\n",
            "The image appears to be a historical document or chart, possibly related to temperature data from 1812-1813. The text is in French and mentions locations such as \"Moscou\" (Moscow) and \"Vilna\" (Vilnius). It might be an old graph showing temperature variations over time for these regions during that period. This kind of information can provide insights into climatic conditions, weather patterns, and historical events that may have affected the areas depicted."
          ]
        }
      ],
      "source": [
        "# Load and preprocess the image\n",
        "image_path = '/mnt/code/test_random.png'  # Replace with the actual image path\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# Define the question and the message structure\n",
        "question = 'What insights can be derived from the image?'\n",
        "msgs = [{'role': 'user', 'content': [image, question]}]\n",
        "\n",
        "# Run the model inference\n",
        "res = model.chat(\n",
        "    image=None,         # Since MiniCPM may not accept images directly, we pass the image within msgs\n",
        "    msgs=msgs,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(f'Assistant Response: {res}')\n",
        "\n",
        "# Optional: Using streaming for generating text\n",
        "generated_text = \"\"\n",
        "for new_text in model.chat(\n",
        "    image=None,\n",
        "    msgs=msgs,\n",
        "    tokenizer=tokenizer,\n",
        "    sampling=True,\n",
        "    stream=True\n",
        "):\n",
        "    generated_text += new_text\n",
        "    print(new_text, flush=True, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1092b4-aa1b-4326-8883-c5a48a54e682",
      "metadata": {
        "id": "7b1092b4-aa1b-4326-8883-c5a48a54e682",
        "outputId": "1eb9b8a3-be0f-4386-af42-7f0b74a33d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant Response: The image is a detailed performance comparison chart for various graphics cards (GCs) running on the RTX 3080 architecture, specifically focusing on their performance in different games at high settings. Here are some key insights derived from this chart:\n",
            "\n",
            "1. **Performance Variability**:\n",
            "   - The chart shows that the performance of different GCs varies significantly across various games. For instance, in \"Anno 1800,\" the Nvidia GeForce RTX 4090 outperforms all other GCs, while in \"Battlefield V,\" the AMD Radeon RX 7900 XT3 leads.\n",
            "   \n",
            "2. **General Trends**:\n",
            "   - Generally, higher-end GCs tend to perform better in most games compared to lower-end ones.\n",
            "   - There are exceptions where mid-range or even entry-level GCs can outperform more expensive options in certain games.\n",
            "\n",
            "3. **Average FPS**:\n",
            "   - An average frame rate (FPS) is provided for each game, giving an overall sense of how well each GC performs across multiple benchmarks.\n",
            "   - The chart also includes an \"AVERAGE\" column showing the mean FPS for each game, which helps in understanding the general performance trend for each GC.\n",
            "\n",
            "4. **Power Consumption and Price**:\n",
            "   - Power consumption data is included, measured in Watts, which indicates how much energy each GC uses during gameplay.\n",
            "   - The price per average frame (PFAF) is listed, reflecting the cost efficiency of each GC based on its performance relative to its price.\n",
            "\n",
            "5. **Ranking**:\n",
            "   - At the bottom, there's a ranking (\"Rank (Best FPS Bang For Buck)\") based on the PFAF score, indicating which GC offers the best balance between performance and price.\n",
            "\n",
            "6. **Comparison Across Different Game Settings**:\n",
            "   - Games are set to highest settings, ensuring that the results reflect real-world gaming scenarios with maximum graphical demands.\n",
            "\n",
            "This chart is useful for gamers and tech enthusiasts looking to make informed decisions about which graphics card would be best suited for their specific gaming needs, considering factors like performance, power usage, and cost.\n",
            "The image is a detailed comparison chart for the RTX 3080 graphics card, showcasing its performance across various games and benchmarks compared to other Nvidia models. Here are some insights derived from the chart:\n",
            "\n",
            "1. **Benchmark Performance**:\n",
            "   - The RTX 3080 generally outperforms other Nvidia cards in most benchmark tests, as indicated by higher scores (green bars).\n",
            "   - It achieves an average boost clock speed of 1879 MHz, which is significantly higher than other models.\n",
            "   - In terms of frame rates, the RTX 3080 often reaches or exceeds 60 FPS in many games, indicating smooth gameplay.\n",
            "\n",
            "2. **Game-Specific Performance**:\n",
            "   - In games like Anno 1800, Assassin’s Creed Odyssey, Battlefield V, Borderlands 3, Civilization VI, Death Stranding, Detroit: Become Human, Devil May Cry 5, Doom Eternal, Far Cry 5, Gears 5, Hitman 2, Metro Exodus, Project Cars 3, Red Dead Redemption 2, Sekiro: Shadows Die Twice, Shadow of the Tomb Raider, Star Wars: Jedi Fallen Order, Strange Brigade, The Witcher 3, it consistently shows high performance with frame rates mostly above 60 FPS and sometimes exceeding 144 FPS.\n",
            "   - Games like Death Stranding show a slight decrease in performance compared to other titles, possibly due to specific optimizations or game engine limitations.\n",
            "\n",
            "3. **Power Consumption**:\n",
            "   - The RTX 3080 has a power consumption of 372W, making it one of the more power-hungry cards listed but still within the range typical for high-performance GPUs.\n",
            "\n",
            "4. **Price and Value**:\n",
            "   - The price per average frame rate at $3.81 is relatively low, suggesting good value for money.\n",
            "   - Compared to other models, the RTX 3080 offers better performance relative to its price, especially when considering the higher frame rates and power efficiency.\n",
            "\n",
            "5. **Ranking**:\n",
            "   - Based on FPS Bang For Buck, the RTX 3080 ranks third, indicating that while it's highly capable, there are other options that offer slightly better value for performance.\n",
            "\n",
            "This chart provides a comprehensive overview of the RTX 3080's capabilities, highlighting its strengths in gaming performance and its position in the market compared to other Nvidia models."
          ]
        }
      ],
      "source": [
        "# Load and preprocess the image\n",
        "image_path = '/mnt/code/test_table.jpg'  # Replace with the actual image path\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# Define the question and the message structure\n",
        "question = 'What insights can be derived from the image? Describe in detail your findings'\n",
        "msgs = [{'role': 'user', 'content': [image, question]}]\n",
        "\n",
        "# Run the model inference\n",
        "res = model.chat(\n",
        "    image=None,         # Since MiniCPM may not accept images directly, we pass the image within msgs\n",
        "    msgs=msgs,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(f'Assistant Response: {res}')\n",
        "\n",
        "# Optional: Using streaming for generating text\n",
        "generated_text = \"\"\n",
        "for new_text in model.chat(\n",
        "    image=None,\n",
        "    msgs=msgs,\n",
        "    tokenizer=tokenizer,\n",
        "    sampling=True,\n",
        "    stream=True\n",
        "):\n",
        "    generated_text += new_text\n",
        "    print(new_text, flush=True, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f60e94-728e-4877-9553-60a1f80e8ae8",
      "metadata": {
        "id": "76f60e94-728e-4877-9553-60a1f80e8ae8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}